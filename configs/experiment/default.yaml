# Data Loading
data:
  dataset: wikitext-103
  split: train
  tokenizer: gpt2
  max_length: 1024

# Training Loop
trainer:
  seed: 1337
  num_iterations: 10000
  validate_every: 500

# Evaluation
evaluation:
  metrics:
    - perplexity
    - loss
  split: validation
  batch_size: 8
  max_samples: 1000

# Logging
logging:
  wandb:
    enabled: false
    project: gpt2-wikitext
    entity: null
